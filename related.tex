% $Id: conclusion.tex 
% !TEX root = main.tex

%%
\section{Related Work}
\label{sec:related}

\ac{RL} inherently deals with the critical trade-off between exploration and exploitation, which 
significantly influences the performance of the learned policy. An agent must explore sufficiently to 
avoid settling for suboptimal solutions, yet excessive exploration can lead to inefficient training. 
Hence, determining an optimal balance between these two strategies is crucial for achieving 
high-quality solutions~\cite{sutton18}.

Many adaptive strategies have been proposed to manage this balance dynamically.~\citet{tokic2010} 
and~\citet{mignon2017adaptive} introduce adaptive implementations of the classic 
$\varepsilon$-greedy policy, highlighting the effectiveness of dynamically adjusting $\varepsilon$ values 
rather than maintaining them statically.~\citet{mignon2017adaptive} demonstrate how an adaptive 
approach enhances performance in both stationary and non-stationary environments by employing 
the Page-Hinkley Test (PH-test) for detecting environmental concept drifts.

Building on the exploration-exploitation dilemma, \citet{norman2024firstexploreexploitmetalearningsolve} 
propose First-Explore, a meta-\ac{RL} approach utilizing distinct policies dedicated to exploration 
and exploitation. Unlike conventional methods that directly optimize cumulative rewards, First-Explore 
trains these two separate policies and later combines them to form an inference policy that strategically 
explores initially for $k$ episodes, sacrificing immediate rewards for greater cumulative future gains. 
This strategy specifically addresses the limitations faced by existing methods that tend to prematurely 
converge to suboptimal solutions due to inadequate early exploration.

Hyperparameter optimization, particularly the learning rate, also plays a crucial role in training 
effective \ac{RL} models. A well-known example is the Adam 
optimizer~\cite{kingma2017adammethodstochasticoptimization}, which dynamically adjusts the learning 
rate based on past gradient information to facilitate more efficient training convergence. Extending 
this line of research, \citet{dynamicrlalpha} propose a dynamic learning rate approach tailored for deep 
\ac{RL} scenarios. Their method adaptively selects optimal learning rates at different training stages, 
demonstrating substantial performance improvements by considering the non-stationarity inherent to 
\ac{RL} tasks.

Transfer learning strategies, like the Transferred Q-Learning~\cite{chen2022transferredqlearning}, 
have demonstrated improvements in convergence rates. By reusing previous knowledge from similar 
tasks, showing that these methods effectively accelerate the learning process.

The methods mentioned above—focused on the adaptivity of learning parameters and the reuse of prior knowledge—closely align with the concept of \ac{CRL}~\cite{khetarpal2022continualreinforcementlearningreview}.
\citet{abel2023definitioncontinualreinforcementlearning} define \ac{CRL} as a setting in which “the best agents never stop learning,” contrasting this with traditional \ac{RL}, which typically treats learning as the identification of a static solution rather than a process of continuous adaptation.
Recent work has begun to explore \ac{CRL} specifically through the lens of Q-learning.
\citet{Bagus2022} provide a systematic empirical studies of Continual Q-learning, using a decomposition of the original task into overlapping but non-contradictory sub-tasks to evaluate the effectiveness of continual learning mechanisms.
Another approach for Continual Q-Learning comes from \citet{araujo2020controladaptiveqlearning}, who propose Adaptive Q-Learning (AQL) and its single-partition variants SPAQL and SPAQL-TS. These algorithms dynamically refine state–action partitions during training and demonstrate strong sample efficiency in continuous control problems such as CartPole, without relying on fixed discretizations.

Various studies have shown \ac{RL} as a potent tool for self-adaptive systems in real-world 
applications~\cite{HENRICHS2022106940}. Notable examples of \ac{RL} implementations 
include the use of adaptive $\varepsilon$-greedy policies to dynamically adjust exploration-exploitation 
trade-offs for \ac{IOT} security in edge computing~\cite{iotdynamicrl}, the integration of \ac{RL} 
with active learning and concept drift detection mechanisms (like PH-Test) for network monitoring to 
effectively identify potential threats~\cite{networkdynamicrl}, or the use of \ac{RL} macro actions to 
continuously learn adaptation strategies~\cite{cardozo23}.

Our approach unifies and extends three key research directions identified in the state of the art: (i) decoupling exploration and exploitation through specialized policies; (ii) online adaptation of learning parameters based on performance feedback; and (iii) proactive environment monitoring and concept drift detection to trigger adaptive responses. Uniquely, \adaptiverl integrates these mechanisms within a \ac{CRL} framework that responds to detected drifts and action-space expansions, resetting exploration and updating learning parameters only when necessary. This coordinated strategy enables continual adaptation without excessive retraining. Furthermore, addressing challenges such as those highlighted by \citet{Bagus2022}, our method mitigates catastrophic forgetting by preserving and reusing prior policy knowledge across evolving configurations, ensuring accelerated convergence even in the presence of overlapping contradictory subtasks.  

\endinput
