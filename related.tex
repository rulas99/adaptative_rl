% $Id: related.tex 
% !TEX root = main.tex

%%
\section{Related Work}
\label{sec:related}

Learning in non-stationary environments is a significant challenge in \ac{RL}, revolving around the exploration-exploitation trade-off~\cite{sutton18}. Our work builds on established research lines addressing this issue, comprehensively surveyed by \citet{Padakandla_2021}.

\subsection{Adapting to Environmental Changes}

A common strategy in non-stationary environments is adapting learning hyperparameters. Several works adjust the exploration rate, $\varepsilon$, often using a change-point detector to trigger the adaptation. Both~\citet{mignon2017adaptive} and~\citet{changingpointdetection} used the PH-test to detect drifts and adapt exploration, a combination our work adopts. Other model-free approaches include Repeated Update Q-learning (RUQL), which addresses policy-bias by repeating updates for less-chosen actions~\cite{nonstationaryqlearning}, contrasting our method's adaptation of the learning rate $\alpha^*$ based on TD-error magnitude. Proactive methods like RestartQ-UCB periodically reset memory on a fixed schedule~\cite{pmlr-v139-mao21b}, whereas \adaptiverl is reactive, using an online PH-test to trigger adaptation only when performance changes significantly. Context-based methods like Context Q-learning~\cite{Padakandla_2020} maintain separate Q-tables for each environmental context, isolating knowledge at the cost of increased memory. \adaptiverl follows a more lightweight philosophy, maintaining a single Q-table and adapting in-place by dynamically scaling the learning rate to integrate new experiences.

\subsection{Continual Learning and Self-Adaptive Systems}
The aforementioned methods align with the principles of \ac{CRL}~\cite{khetarpal2022continualreinforcementlearningreview}, where the goal is for agents to ``never stop learning''~\cite{abel2023definitioncontinualreinforcementlearning}. Recent studies show that many CL methods struggle when a state-action pair yields different rewards after an environmental shift~\cite{Bagus2022}. \adaptiverl addresses this directly: by not resetting its Q-table and forcing re-exploration, an outdated Q-value generates a large TD-error when encountering a new reward, which in turn drives a high learning rate to rapidly overwrite the obsolete knowledge. \ac{RL} is also a potent tool for building self-adaptive systems~\cite{HENRICHS2022106940}, with applications in IoT security~\cite{iotdynamicrl} and network monitoring~\cite{networkdynamicrl}. Our traffic-control scenario is a canonical example of this paradigm.

\subsection{Positioning of Our Contribution}
The main contribution of this paper is not a single algorithmic component, but rather the synthesis and integration of several established techniques into a unified, model-free framework for tabular Q-learning. \adaptiverl combines proactive environment monitoring via the PH-test with reactive adaptation of both the exploration rate ($\varepsilon^*$) and the learning rate ($\alpha^*$). This coordinated strategy is specifically designed to enable an agent to adapt to two distinct types of non-stationarity simultaneously: changes in the reward function (shifting goals) and on-the-fly expansions of the action space. By preserving and building upon a single Q-table, our approach provides a lightweight solution aimed at mitigating catastrophic forgetting and ensuring continual adaptation in dynamic environments, without the need for full retraining or maintaining multiple explicit context models.

\endinput