% $Id: conclusion.tex 
% !TEX root = main.tex

%%
\section{Related Work}
\label{sec:related}

\ac{RL} inherently deals with the critical trade-off between exploration and exploitation, which 
significantly influences the performance of the learned policy. An agent must explore sufficiently to 
avoid settling for suboptimal solutions, yet excessive exploration can lead to inefficient training. 
Hence, determining an optimal balance between these two strategies is crucial for achieving 
high-quality solutions~\cite{sutton98}.

Many adaptive strategies have been proposed to manage this balance dynamically. ~\citet{tokic2010} 
and then~\citet{mignon2017adaptive} introduce adaptive implementations of the classic 
$\epsilon-greedy$ policy, highlighting the effectiveness of dynamically adjusting $\epsilon$ values 
rather than maintaining them statically. ~\citet{mignon2017adaptive} demonstrate how an adaptive 
approach enhanced performance in both stationary and non-stationary environments by employing 
the Page-Hinkley Test (PH-test) for detecting environmental concept drifts.

Building on the exploration-exploitation dilemma, \citet{norman2024firstexploreexploitmetalearningsolve} 
propose First-Explore, a meta-\ac{RL} approach utilizing distinct policies dedicated to exploration 
and exploitation. Unlike conventional methods that directly optimize cumulative rewards, First-Explore 
trains these two separate policies and later combines them to form an inference policy that strategically 
explores initially for $k$ episodes, sacrificing immediate rewards for greater cumulative future gains. 
This strategy specifically addresses the limitations faced by existing methods that tend to prematurely 
converge to suboptimal solutions due to inadequate early exploration.

Hyperparameter optimization, particularly the learning rate, also plays a crucial role in training 
effective \ac{RL} models. A well-known example is the Adam 
optimizer~\cite{kingma2017adammethodstochasticoptimization}, which dynamically adjusts the learning 
rate based on past gradient information to facilitate more efficient training convergence. Extending 
this line of research, \citet{dynamicrlalpha} propose a dynamic learning rate approach tailored for deep 
\ac{RL} scenarios. Their method adaptively selects optimal learning rates at different training stages, 
demonstrating substantial performance improvements by considering the non-stationarity inherent to 
\ac{RL} tasks.

Transfer learning strategies, like the Transferred Q-Learning~\cite{chen2022transferredqlearning}, 
have demonstrated improvements in convergence rates. By reusing previous knowledge from similar 
tasks, showing that these methods effectively accelerate the learning process.

The methods mentioned above, related to the adaptivity of learning parameters and the reuse of 
previous knowledge, closely align with the concept of 
\ac{CRL}~\cite{khetarpal2022continualreinforcementlearningreview}. 
\citet{abel2023definitioncontinualreinforcementlearning} define \ac{CRL} as a setting in which "the 
best agents never stop learning," contrasting this perspective with traditional \ac{RL} approaches, 
which typically treat learning as the identification of a static solution rather than an ongoing adaptive 
process.

Various studies have shown \ac{RL} as a potent tool for self-adaptive systems in real-world 
applications~\cite{HENRICHS2022106940}. Notable examples of \ac{RL} implementations 
include the use of adaptive $\epsilon-greedy$ policies to dynamically adjust exploration-exploitation 
trade-offs for \ac{IOT} security in edge computing~\cite{iotdynamicrl}, the integration of \ac{RL} 
with active learning and concept drift detection mechanisms (like PH-Test) for network monitoring to 
effectively identify potential threats~\cite{networkdynamicrl}, or the use of \ac{RL} macro actions to 
continuously learn adaptation strategies~\cite{cardozo23}.

Our approach compares with the state-of-the-art in that 


\endinput

