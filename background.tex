% $Id: conclusion.tex 
% !TEX root = main.tex

%%
\section{Background}
\label{sec:background}

%%%%
\subsection{\acl{RL}}

In \ac{RL}, agents learn to map environmental situations (environment states) to actions that maximize a numerical reward signal received from the environment in the long term~\cite{sutton18}. \ac{RL} problems are formulated as Markov Decision Processes (MDPs), defined by the tuple $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, where:
$\mathcal{S}$ is the state space, comprising all relevant states of the environment;
$\mathcal{A}$ is the action space, i.e., the set of all actions the agent can perform to affect the environment;
$P(s_{t+1} \mid s_t, a_t)$ is the transition probability from state {\color{purple}$s_t$} to {\color{purple}$s_{t+1}$} under action {\color{purple}$a_t$};
$R(s_t, a_t)$ is the reward function, providing the numerical signal ($r$) that encodes the positive or negative impact of taking action {\color{purple}$a_t$} in state {\color{purple}$s_t$} at each execution step $t$; and $\gamma \in [0, 1]$ is the discount factor, which determines the importance of future rewards.

In stationary settings, $P$ and $R$ remain fixed over time, and the agentâ€™s objective is to learn a policy $\pi: \mathcal{S} \to \mathcal{A}$ that maximizes the expected discounted return. However, real-world environments often violate this stationarity assumption: transition dynamics and/or reward functions may shift due to evolving conditions. We model such scenarios as \emph{non-stationary} MDPs, represented by a sequence
\[
\{\mathcal{M}_t\}_{t=1}^\infty
\quad\text{with}\quad
\mathcal{M}_t = \bigl\langle \mathcal{S}, \mathcal{A}_t, P_t, R_t, \gamma \bigr\rangle,
\]
where $P_t$ and/or $R_t$ change at unknown time steps $t$ (``concept drifts''). In this work, we further consider the novel scenario where the agent's action space $\mathcal{A}$ may also change over time. Detecting and adapting to these drifts is central to \ac{CRL}~\cite{khetarpal2022continualreinforcementlearningreview,abel2023definitioncontinualreinforcementlearning}, which treats learning as an ongoing process across a continually changing sequence of tasks.

Q-learning~\cite{watkins92} is a widely used model-free approach in \ac{RL}. The long-term quality of an action performed at a given state is computed iteratively in a series of steps and is represented by a Q value,
$\mathit{Q(s,a)}$.
Formally, each execution step $t$ captures information from the environment and maps it to a state
{\color{purple}$s_t$} $\in \mathcal{S}$ in its state space. It then selects an
action {\color{purple}$a_t$} $\in \mathcal{A}$ from its action space and executes it. The agent
receives a reward {\color{Bittersweet}$r_t$} from the environment when it moves to the next state
$s_{t+1} \in \mathcal{S}$. The reward is used to update the optimality of performing the
action {\color{Mulberry}$a_t$} at state {\color{purple}$s_t$}. The agent's goal is to learn
a policy (\i.e. the best-fit action for each state) that maximizes the reward of the
long-run behavior. The learning rate \lrate{\alpha} determines how much new experiences 
overwrite previously learned experiences, and the discount factor {\color{RoyalBlue} $\gamma$} 
determines how much future rewards are discounted so that agents prioritize immediate actions and 
can plan the best long-term actions. At each time step $t$, the Q value of an action 
{\color{purple}$a_{t+1}$} taken in state {\color{purple}$s_{t+1}$}, $Q(s_{t+1}, a_{t+1})$, is updated by 
the Bellman learning equation as follows:

\vspace{1em}
\input{equations/bellman}
\vspace{1em}

While Q-learning converges under stationary conditions (with appropriate decay of \lrate{\alpha}), it 
can struggle when $P_t$ or $R_t$ change over time. Our work builds on this foundation by 
incorporating online concept-drift detection and adaptive updates, enabling the agent to remain 
effective in non-stationary MDPs. We focus particularly on shifts in the reward function $R_t$, which 
may also induce changes in the action space $\mathcal{A}_t$.


%%%%
\subsection{Motivating example}
\label{sec:motivation}


To motivate the adaptation of agent's behavior to changing goals and the acquisition of new actions, 
we use two scenarios of the Gridworld benchmark as a running example.

Gridworld consists of a rectangular matrix ($9\times 9$ in our case). In Gridworld, every cell is 
associated with a reward of $-1$, except for a single goal state that has a reward of $+100$ as shown 
in the left-hand side of \fref{fig:r-change}. The agent begins each episode at the grid's center and may 
move in four directions: \spy{up}, \spy{down}, \spy{left}, or \spy{right}. Initially, the goal is placed in the 
top-left corner.  

In the first scenario, we allow the goal state of the environment to change. Such changes are unknown 
to the agents beforehand~\cite{cardozo21} and maybe due to the reallocation of 
objectives~\cite{khetarpal2022continualreinforcementlearningreview} (\eg reorganizing products in a 
warehouse for robotic fulfillment systems), or the shift to new objectives~\cite{florensa18} (\eg UAV drones 
changing their mission due to flight plan changes, or changing in leadership for flight formations).

\begin{figure*}[hptb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/rewards_change}
    \caption{Non-Stationary Gridworld: Concept drift is induced by relocating the reward. The agent starts at the center and must reach the goal, which alternates between the top-left and bottom-right corners every 300 episodes.}
    \label{fig:r-change}
\end{figure*}

Additionally, we consider a scenario of adding walls to the Gridworld, blocking the agent's movement 
according to its initial set of actions (\fref{fig:q-value-comp2}). For such scenario, we enable the agent 
to  extended its action space by acquiring new actions that provide additional capabilities to the agent 
(\ie jumping). Such situations are common in robotics for modular/evolving 
robots~\cite{eiben20evolving,miras20environmental}, or swarm behavior~\cite{schranz20swarm} where 
robots acquire new capabilities due to the physical attachment of parts, the possibility of combining 
actions with nearby robots, or the transfer of responsibilities/behavior between robots. Whichever the 
case may be, the agent is expected to adapt to these changes and efficiently adjust its policy using 
the newly acquired behavior.


\endinput

