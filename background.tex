% $Id: background.tex 
% !TEX root = main.tex

%%
\section{Background}
\label{sec:background}

%%%%
\subsection{\acl{RL} in Non-Stationary Environments}

\ac{RL} agents learn optimal policies by interacting with environments formulated as \acp{MDP} $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, where $\mathcal{S}$ and $\mathcal{A}$ are the state and action spaces, $P$ represents transition probabilities, $R$ is the reward function, and $\gamma$ is the discount factor~\cite{sutton18}.

Real-world environments often violate stationarity assumptions, leading to \emph{non-stationary} MDPs represented as temporal sequences:
\[
\{\mathcal{M}_t\}_{t=1}^\infty \quad\text{with}\quad \mathcal{M}_t = \bigl\langle \mathcal{S}, \mathcal{A}_t, P_t, R_t, \gamma \bigr\rangle
\]
where changes in $R_t$ and $\mathcal{A}_t$ constitute \emph{concept drifts}. Q-learning updates action values using the Bellman equation:
\vspace{1em}
\input{equations/bellman}
\vspace{1em}

While effective in stationary settings, standard Q-learning struggles when $R_t$ or $\mathcal{A}_t$ change over time. Our work addresses this limitation through adaptive mechanisms that detect and respond to environmental changes in the form of modifications to the reward distribution function and action space expansions. We focus on rewards and action spaces, as changes in the state space has been addressed in the literature~\cite{ConstructivistRL}.

%%%%
\subsection{Motivating Example}
\label{sec:motivation}
We motivate our work using a $9\times 9$ Gridworld benchmark, where agents navigate from the center of the board to a goal state (\ie a state with reward $+100$) while avoiding negative rewards (states with reward $-1$). The environment presents two adaptation challenges:
\begin{itemize}
    \item Goal Relocation: The target switches between corners, from the top-left corner to the bottom 
    right-corner. This requiring agents to adapt to new reward distributions without losing previously 
    learned knowledge, as the goal changes multiple times. In each change, the agent should take less 
    time in learning the behavior to reach the goal, than the required time to learn from scratch.
    \item Action Space Expansion: New actions (\eg jumping over obstacles) are introduced, requiring 
    agents to integrate new capabilities into existing policies to further optimize the policy with the new 
    actions.
\end{itemize}


\endinput


For the traffic signal control scenario, agents must adapt to changing traffic patterns and incorporate new actions like adjusting signal timings or phases. These scenarios reflect real-world applications like robotic systems adapting to changing objectives or acquiring new capabilities through modular components.
