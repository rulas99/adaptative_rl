% $Id: validation.tex 
% !TEX root = main.tex

%%
\section{Validation}
\label{sec:validation}

We evaluate \adaptiverl in two scenarios: a canonical Gridworld benchmark for a clear proof-of-concept, and a more real application over traffic-signal control simulation to validate its adaptability in a scenario representative of real-world self-adaptive systems.

\subsection{Evaluation Scenarios}
Gridworld: In a $9 \times 9$ grid, we run two experiments for 1,000 independent trials each. First, in a 1,500-episode run, the high-reward goal state is swapped between opposite corners every 300 episodes. Second, in a 400-episode run, a new ``jump'' action is introduced at episode 300, requiring the agent to find a more optimal path.

Traffic-Signal Control: We model a two-lane intersection using a custom extended Gym environment~\cite{gymlib}, a well known application for SAS~\cite{HENRICHS2022106940}. The state $s_t = (c_1, c_2)$ is the number of queued vehicles per lane. Actions are signal phases with different service capacities. The reward function penalizes both congestion (queues exceeding a threshold) and inefficiency (allocating green time to empty lanes). Concept drift is induced by changing vehicle arrival rates ($\lambda_1, \lambda_2$) at predefined episodes. Upon drift detection, the action space is expanded with more aggressive signal phases to manage heavier traffic.

\subsection{Experimental Setting}
Experiments were run using Python 3.12 and Gym 0.26.2. We compare \adaptiverl against a Standard Q-learning (Baseline) agent with a fixed learning rate ($\alpha=0.1$) and a single, non-resetting exponential exploration decay. \adaptiverl uses the PH-test to trigger exploration resets ($\varepsilon \to 1$) and a dynamic learning rate $\alpha^*$ modulated by the TD-error. Hyperparameters for both scenarios (e.g., $k=5, \delta=0.5, H=300$) were determined empirically to suit the reward scale of each environment.
\subsection{Evaluation Results}
Gridworld: \adaptiverl demonstrates superior adaptation and knowledge retention. As illustrated in \fref{fig:q-heatmaps-goals}, after 1,500 episodes, \adaptiverl (left) successfully retains high Q-values for both current and previously learned goals, effectively reducing catastrophic forgetting effects. In contrast, the baseline agent (right) overwrites past knowledge and only remembers the most recent goal. This enhanced adaptability translates to significant efficiency gains, as shown in \fref{tab:gridworld-table}. \adaptiverl improves overall learning efficiency by a factor of 1.7x (measured in total steps) and successfully converges after each induced drift. The baseline agent, however, fails to converge after the first 300-episode interval. Furthermore, \fref{fig:q-heatmaps-actions} shows that when the action space is expanded, \adaptiverl effectively incorporates the new ``jump'' action to discover a more optimal policy, whereas the baseline agent struggles to update its established policy and remains suboptimal.

Traffic-Signal Control: In this scenario, \adaptiverl again shows robust adaptation (\fref{fig:traffic-learning-curve}). When congestion increases at episode 3,000, the PH-test detects the drift, triggering adaptation and enabling a rapid performance recovery. The baseline agent suffers a prolonged degradation. However, the results also highlight a limitation: a second drift at episode 8,000 (lowered traffic) is not detected because the new reward distribution is a subset of previously seen values and does not exceed the PH-test's sensitivity threshold. While both agents performance improves in the easier environment, this failure underscores the challenge of parameterizing drift detectors. Despite this, the case study confirms \adaptiverl's ability to react to detected non-stationarity and seamlessly incorporate new actions to manage changing conditions.

\begin{table}[h]
\centering
\caption{Average convergence time (in episodes after drift) and total steps over 1,500 episodes in Gridworld (1,000 runs). Dashes (--) indicate failure to converge within the 300-episode interval. An independent t-test confirms the difference in total steps is statistically significant ($p < 0.05$).}
\label{tab:gridworld-table}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l | c | c | c | c }
\toprule
\textbf{Agent} & \textbf{1st Drift} & \textbf{2nd Drift} & \textbf{3rd Drift} & \textbf{Total Steps} \\
\midrule
Q-Learning  & 256.40 $\pm$ 4.35\% & -- & -- & 40,683.74 $\pm$ 1.33\% \\
\textbf{\adaptiverl} & 135.81 $\pm$ 9.31\% & 175.17 $\pm$ 5.80\% & 167.81 $\pm$ 3.13\% & 23,292.07 $\pm$ 6.33\% \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/q_map_comp.png}
        \caption{Adaptation to shifting goals (1,500 episodes).}
        \label{fig:q-heatmaps-goals}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/q_map_comp2.png}
        \caption{Adaptation to an expanded action space (400 episodes).}
        \label{fig:q-heatmaps-actions}
    \end{subfigure}
    
    \caption{Comparison of Q-value heatmaps demonstrating \adaptiverl's superior adaptation over standard Q-learning in two non-stationary scenarios. (a) In a goal-switching environment, \adaptiverl (left) preserves high Q-values for both initial and subsequent goals, showcasing effective knowledge retention and preventing catastrophic forgetting, unlike the baseline agent (right). (b) When the action space is expanded, \adaptiverl (left) successfully integrates a new ``jump'' action (indicated by double arrows) to discover a more optimal policy, while the baseline (right) fails to adapt, remaining committed to a suboptimal policy.}
    \label{fig:q-heatmaps-combined}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{minipage}{0.7\textwidth}
        \includegraphics[width=\textwidth]{figures/traffic_learning_curve.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.25\textwidth}
        \caption{Learning performance in the traffic scenario. After the first drift (episode 3,000), \adaptiverl detects the change (green line) and rapidly recovers. Traditional Q-learning suffers prolonged degradation. The second drift (episode 8,000) is not detected by the PH-tests because the new reward distribution is a subset of previously seen values and does not exceed the sensitivity threshold.}
        \label{fig:traffic-learning-curve}
    \end{minipage}
\end{figure*}

\endinput