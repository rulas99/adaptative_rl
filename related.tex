% $Id: conclusion.tex 
% !TEX root = main.tex

%%
\section{Related Work}
\label{sec:related}

Reinforcement Learning (RL) inherently deals with the critical trade-off between exploration and exploitation, which significantly influences the performance of the learned policy. As stated by \citet{sutton98}, an agent must explore sufficiently to avoid settling on suboptimal solutions, yet excessive exploration can lead to inefficient training. Hence, determining an optimal balance between these two strategies is crucial for achieving high-quality solutions.

Several adaptive strategies have been proposed to manage this balance dynamically. \citet{mignon2017adaptive} introduced an adaptive implementation of the classic ε-greedy policy, highlighting the effectiveness of dynamically adjusting ε values rather than maintaining them statically. They particularly demonstrated how an adaptive approach enhanced performance in both stationary and non-stationary environments by employing the Page-Hinkley Test (PH-test) for detecting environmental concept drifts.

Building on the exploration-exploitation dilemma, \citet{norman2024firstexploreexploitmetalearningsolve} proposed First-Explore, a meta-RL approach utilizing distinct policies dedicated to exploration and exploitation. Unlike conventional methods that directly optimize cumulative rewards, First-Explore trains these two separate policies and later combines them to form an inference policy that strategically explores initially, sacrificing immediate rewards for greater cumulative future gains. This strategy specifically addresses the limitations faced by existing methods that tend to prematurely converge to suboptimal solutions due to inadequate early exploration.

Hyperparameter optimization, particularly the learning rate, also plays a crucial role in training effective RL models. A well-known example is the Adam optimizer introduced by \citet{kingma2017adammethodstochasticoptimization}, which dynamically adjusts the learning rate based on past gradient information to facilitate more efficient training convergence. Extending this line of research, \citet{dynamicrlalpha} proposed a bandit-based dynamic learning rate approach tailored for deep RL scenarios. Their method adaptively selects optimal learning rates at different training stages, demonstrating substantial performance improvements by considering the non-stationarity inherent to RL tasks.

Moreover, transfer learning strategies, like the Transferred Q-Learning proposed by \citet{chen2022transferredqlearning}, have shown significant improvements in the convergence rate when leveraging data from similar RL tasks. Their approach emphasizes the vertical information cascading along multiple RL task stages and provides theoretical guarantees demonstrating faster convergence rates and reduced regret bounds when applying transfer learning under similarity assumptions.

These methods align closely with the broader concept of Continual Reinforcement Learning (CRL). \citet{khetarpal2022continualreinforcementlearningreview} define CRL as a setting where "the best agents never stop learning," contrasting with traditional RL approaches, which typically treat learning as finding a static solution rather than an ongoing adaptive process.

Accordingly, various studies have shown RL as a potent tool for self-adaptive systems in real-world applications~\cite{HENRICHS2022106940}. Notable examples include \citet{iotdynamicrl}, who employed adaptive ε-greedy policies to dynamically adjust exploration-exploitation trade-offs for IoT security in edge computing, and \citet{networkdynamicrl}, who integrated RL with active learning and concept drift detection mechanisms for network monitoring to effectively identify potential threats.

Finally, our research applies these advanced RL methodologies within the domain of Intelligent Traffic Management—particularly in traffic signal control, which is an established application area for self-adaptive systems~\cite{HENRICHS2022106940}. Notably, \citet{meta-rl-traffic} addressed traffic signal control non-stationarity through a meta-RL model that dynamically switches reward functions based on traffic flow saturation levels. Furthermore, recent works by \citet{Swapno2024} and \citet{MORENOMALO2024124178} have successfully demonstrated the effectiveness of Deep Q-Networks (DQN) in optimizing traffic signal control, achieving substantial improvements in terms of reduced waiting times and queue lengths. Our approach is inspired by these models, utilizing a similar state-action space configuration based on the number of vehicles per lane, with phase durations as defined actions, thus contributing further to efficient and sustainable urban mobility.



\endinput

