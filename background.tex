% $Id: background.tex 
% !TEX root = main.tex

%%
\section{Background}
\label{sec:background}

%%%%
\subsection{\acl{RL} in Non-Stationary Environments}

\ac{RL} agents learn optimal policies by interacting with environments formulated as \acp{MDP} $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, where $\mathcal{S}$ and $\mathcal{A}$ are the state and action spaces, $P$ represents transition probabilities, $R$ is the reward function, and $\gamma$ is the discount factor~\cite{sutton18}.

Real-world environments often violate stationarity assumptions, leading to \emph{non-stationary} MDPs represented as temporal sequences:
\[
\{\mathcal{M}_t\}_{t=1}^\infty \quad\text{with}\quad \mathcal{M}_t = \bigl\langle \mathcal{S}, \mathcal{A}_t, P_t, R_t, \gamma \bigr\rangle
\]
where changes in $R_t$ and $\mathcal{A}_t$ constitute \emph{concept drifts}. Q-learning updates action values using the Bellman equation:
\vspace{1em}
\input{bellman}
\vspace{1em}

While effective in stationary settings, standard Q-learning struggles when $R_t$ or $\mathcal{A}_t$ change over time. Our work addresses this limitation through adaptive mechanisms that detect and respond to environmental changes in the form of modifications to the reward distribution function and action space expansions. Note that changes in the state space have already been studied by~\citet{ConstructivistRL}.

%%%%
\subsection{Motivating Example}
\label{sec:motivation}
We demonstrate our approach using a $9\times 9$ Gridworld where agents navigate from the center to a goal state (reward $+100$) while avoiding negative rewards ($-1$). The environment presents two adaptation challenges:
\begin{itemize}
    \item Goal Relocation: The target switches between corners, requiring agents to adapt to new reward distributions without losing previously learned knowledge.
    \item Action Space Expansion: New actions (e.g., jumping over obstacles) are introduced, requiring agents to integrate new capabilities into existing policies while maintaining performance.
\end{itemize}

For the traffic signal control scenario, agents must adapt to changing traffic patterns and incorporate new actions like adjusting signal timings or phases. These scenarios reflect real-world applications like robotic systems adapting to changing objectives or acquiring new capabilities through modular components.

\endinput
