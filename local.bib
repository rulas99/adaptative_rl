@comment{----2024 ---}


@misc{abel2023definitioncontinualreinforcementlearning,
  	title={A Definition of Continual Reinforcement Learning},
  	author={David Abel and André Barreto and Benjamin Van Roy and Doina Precup and Hado van Hasselt and Satinder Singh},
  	year={2023},
  	eprint={2307.11046},
  	archivePrefix={arXiv},
  	primaryClass={cs.LG},
  	url={https://arxiv.org/abs/2307.11046},
}

@article{HENRICHS2022106940,
title = {A literature review on optimization techniques for adaptation planning in adaptive systems: State of the art and research directions},
journal = {Information and Software Technology},
volume = {149},
pages = {106940},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106940},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000891},
author = {Elia Henrichs and Veronika Lesch and Martin Straesser and Samuel Kounev and Christian Krupitzer},
keywords = {Self-adaptive systems, Adaptation planning, Optimization, Survey},
}

@misc{khetarpal2022continualreinforcementlearningreview,
      title={Towards Continual Reinforcement Learning: A Review and Perspectives}, 
      author={Khimya Khetarpal and Matthew Riemer and Irina Rish and Doina Precup},
      year={2022},
      eprint={2012.13490},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.13490}, 
}

@misc{chen2022transferredqlearning,
      title={Transferred Q-learning}, 
      author={Elynn Y. Chen and Michael I. Jordan and Sai Li},
      year={2022},
      eprint={2202.04709},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.04709}, 
}

@misc{beck2024surveymetareinforcementlearning,
      title={A Survey of Meta-Reinforcement Learning}, 
      author={Jacob Beck and Risto Vuorio and Evan Zheran Liu and Zheng Xiong and Luisa Zintgraf and Chelsea Finn and Shimon Whiteson},
      year={2024},
      eprint={2301.08028},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.08028}, 
}


@article{meta-rl-traffic,
author = {Kim, Gyeongjun and Kang, Jiwon and Sohn, Keemin},
year = {2022},
month = {09},
pages = {},
title = {A meta–reinforcement learning algorithm for traffic signal control to automatically switch different reward functions according to the saturation level of traffic flows},
volume = {38},
journal = {Computer-Aided Civil and Infrastructure Engineering},
doi = {10.1111/mice.12924}
}

@article{MORENOMALO2024124178,
title = {Improving traffic light systems using Deep Q-networks},
journal = {Expert Systems with Applications},
volume = {252},
pages = {124178},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124178},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424010443},
author = {Juan Moreno-Malo and Juan-Luis Posadas-Yagüe and Juan Carlos Cano and Carlos T. Calafate and J. Alberto Conejero and Jose-Luis Poza-Lujan},
keywords = {Deep-Q networks, Urban traffic optimization, Neural network, SUMO},
abstract = {As our cities become more complex and traffic demand grows, managing such traffic efficiently becomes challenging. Hence, solutions that allow building upon the current traffic light systems and that can be readily deployed are of global interest. In this work, we address the challenge of improving traffic light management at intersections. We propose an agent-based traffic light control system where an agent, one per intersection, dynamically regulates the light’s phase cycle depending on the current traffic conditions. To this end, we will rely on Deep Networks to adequately train agents to make good decisions. Simulation results in a realistic scenario using SUMO show that our proposed approach can significantly reduce waiting times, improving transit times by 44% compared to the standard fixed-timing method. Additionally, to assess the effectiveness and reliability of our control algorithm, we introduce new performance metrics.}
}

@inproceedings{tokic2010,
author = {Tokic, Michel},
year = {2010},
month = {09},
pages = {203-210},
title = {Adaptive ε-Greedy Exploration in Reinforcement Learning Based on Value Differences},
isbn = {978-3-642-16110-0},
doi = {10.1007/978-3-642-16111-7_23}
}

@article{mignon2017adaptive,
author = {Mignon, Alexandre and A. Rocha, Ricardo Luis},
year = {2017},
month = {12},
pages = {1146-1151},
title = {An Adaptive Implementation of ε-Greedy in Reinforcement Learning},
volume = {109},
journal = {Procedia Computer Science},
doi = {10.1016/j.procs.2017.05.431}
}

@inproceedings{inproceedings,
author = {Wassermann, Sarah and Cuvelier, Thibaut and Mulinka, Pavol and Casas, Pedro},
year = {2019},
month = {10},
pages = {},
title = {ADAM & RAL: Adaptive Memory Learning and Reinforcement Active Learning for Network Monitoring},
doi = {10.23919/CNSM46954.2019.9012675}
}

@article{article,
author = {Wassermann, Sarah and Cuvelier, Thibaut and Mulinka, Pavol and Casas, Pedro},
year = {2020},
month = {11},
pages = {1-1},
title = {Adaptive and Reinforcement Learning Approaches for Online Network Monitoring and Analysis},
volume = {PP},
journal = {IEEE Transactions on Network and Service Management},
doi = {10.1109/TNSM.2020.3037486}
}

@article{kumar2024adaptive,
author = {Kumar, Anit and Singh, Dhanpratap},
year = {2024},
month = {11},
pages = {},
title = {Adaptive epsilon greedy reinforcement learning method in securing IoT devices in edge computing},
volume = {4},
journal = {Discover Internet of Things},
doi = {10.1007/s43926-024-00080-7}
}

@inproceedings{10.1609/aaai.v37i6.25899,
author = {Ding, Wei and Jiang, Siyang and Chen, Hsi-Wen and Chen, Ming-Syan},
title = {Incremental reinforcement learning with dual-adaptive ε-greedy exploration},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i6.25899},
doi = {10.1609/aaai.v37i6.25899},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {830},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}