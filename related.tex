% $Id: related.tex 
% !TEX root = main.tex

%%
\section{Related Work}
\label{sec:related}

The challenge of learning in non-stationary environments is a significant area of research in \ac{RL}. A central issue is balancing the exploration-exploitation trade-off, where an agent must explore to adapt to changes while exploiting its knowledge to maximize rewards~\cite{sutton18}. Our work, \adaptiverl, builds upon several established lines of research that address this challenge. A comprehensive overview of algorithms for dynamically varying environments can be found in the survey by \citet{Padakandla_2021}.

\subsection{Adapting to Environmental Changes}

A common strategy for non-stationary environments is to adapt the agent's learning hyperparameters in response to change. Several works have focused on dynamically adjusting the exploration rate, $\varepsilon$. For example,~\citet{tokic2010} and~\citet{mignon2017adaptive} proposed adaptive implementations of the $\varepsilon$-greedy policy. A key idea in this area is to use a change-point detector to trigger the adaptation. Both~\citet{mignon2017adaptive} and~\citet{changingpointdetection} demonstrated the use of the Page-Hinkley (PH) test to detect environmental drifts and subsequently adjust the exploration strategy, showing enhanced performance over static approaches. Our work adopts this established combination of a PH-test to trigger a reset of the exploration rate.

Beyond exploration, other model-free approaches have been proposed. \citet{nonstationaryqlearning} introduced Repeated Update Q-learning (RUQL), an algorithm that addresses the policy-bias of Q-learning (where the update frequency depends on the action selection probability) by repeating updates for less-frequently chosen actions. This contrasts with our method, which adapts the learning rate $\alpha^*$ based on the magnitude of the TD-error rather than the policy distribution. \citet{pmlr-v139-mao21b} proposed RestartQ-UCB, which periodically resets the agent's memory based on a pre-calculated schedule to handle non-stationarity. This represents a proactive strategy, whereas \adaptiverl is reactive, using the online PH-test to trigger adaptation only when a significant change in performance is detected.

Another relevant line of work involves context-based methods. For instance, \citet{Padakandla_2020} proposed Context Q-learning, which detects changes and maintains a separate Q-table for each environmental context. This explicitly isolates knowledge for different environment models but at the cost of increased memory. \adaptiverl follows a different design philosophy: it maintains a single Q-table and adapts in-place. Knowledge is preserved not by storing separate models, but by preventing the Q-values from being erased or directly override it and by dynamically scaling the learning rate to control the integration of new experiences, aiming for a more lightweight solution.

\subsection{Continual Learning and Self-Adaptive Systems}
The methods mentioned above—focused on adapting hyperparameters and reusing knowledge—closely align with the principles of Continual Reinforcement Learning (\ac{CRL})~\cite{khetarpal2022continualreinforcementlearningreview}. \citet{abel2023definitioncontinualreinforcementlearning} define \ac{CRL} as a setting where ``the best agents never stop learning'', which is the core challenge in non-stationary environments. Recent work has explored \ac{CRL} through the lens of Q-learning. \citet{Bagus2022} provide systematic empirical studies of continual learning methods for Q-learning, highlighting that many CL approaches are designed for non-contradictory sub-tasks and struggle in realistic RL scenarios where the same state-action pair can yield different rewards after an environmental shift. This is a challenge that \adaptiverl addresses by not resetting the Q-table and instead forcing re-exploration. This mechanism is inherently robust to such contradictory information; when a drift occurs, the outdated Q-value for a given state-action pair generates a large Temporal Difference (TD) error upon encountering the new reward, which in turn drives a high learning rate and allows the new experience to rapidly and effectively overwrite the obsolete knowledge.

\ac{RL} has also been shown to be a potent tool for building self-adaptive systems~\cite{HENRICHS2022106940}. Notable examples include using adaptive $\varepsilon$-greedy policies for \ac{IOT} security~\cite{iotdynamicrl}, integrating \ac{RL} with drift detection for network monitoring~\cite{networkdynamicrl}. Our traffic-control scenario is a canonical example in this domain, where an autonomous agent must continually adapt its control policy.

\subsection{Positioning of Our Contribution}
The main contribution of this paper is not a single, novel algorithmic component, but rather the synthesis and integration of several established techniques into a unified, model-free framework for tabular Q-learning. \adaptiverl combines proactive environment monitoring via the PH-test with reactive adaptation of both the exploration rate ($\varepsilon^*$) and the learning rate ($\alpha^*$). This coordinated strategy is specifically designed to enable an agent to adapt to two distinct types of non-stationarity simultaneously: changes in the reward function (shifting goals) and on-the-fly expansions of the action space. By preserving and building upon a single Q-table, our approach provides a lightweight solution aimed at mitigating catastrophic forgetting and ensuring continual adaptation in dynamic environments, without the need for full retraining or maintaining multiple explicit context models.

\endinput