% !TeX document-id = {4951685f-4d1a-4629-8de6-69a05c8522c0}
%  $Id: main.tex $
% !BIB TS-program = bibtex

\RequirePackage{graphicx}
\documentclass[10pt, conference, ]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\input{preamble}
\input{acronyms}


\begin{document}

\title{Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions}

%\author{
%\IEEEauthorblockN{Raul de la Rosa}
%\IEEEauthorblockA{%\textit{ Department of Computer Science and Technology} \\
%\textit{Universidad de los Andes}\\
%Bogot\'a, Colombia \\
%c.delarosap@uniandes.edu.co}
%\and
%\IEEEauthorblockN{Ivana Dusparic}
%\IEEEauthorblockA{%\textit{School of Computer Science and Statistics} \\
%\textit{Trinity College Dublin}\\
%Dublin, Ireland \\
%ivana.dusparic@tcd.ie}
%\and
%\IEEEauthorblockN{Nicol\'as Cardozo}
%\IEEEauthorblockA{%\textit{Systems and Computing Engineering Department}\\
%\textit{Universidad de los Andes}\\
%Bogot\'a, Colombia \\
%n.cardozo@uniandes.edu.co}
%}
\author{
\IEEEauthorblockN{Author template}
\IEEEauthorblockA{%\textit{ Department of Computer Science and Technology} \\
\textit{University}\\
City, Country \\
university-email}
}

\maketitle




\begin{abstract}
In Reinforcement Learning, agents learn to solve specific tasks through the 
exploration of the environment in which they execute. Agents' behavior is
specialized towards a goal, gathered from the environment, according to the 
agent's experience from the execution of its actions. While effective in optimizing the behavior for a 
goal, the performance of agent's rapidly decreases when the environment conditions change.
This paper introduces \adaptiverl, a self-adaptive Q-learning agent that enables on-the-fly adaptation 
without full retraining. \adaptiverl leverages proactive environment monitoring and concept-drift 
detection to trigger dynamic adjustment of learning and exploration parameters only when needed, 
while preserving prior policy knowledge to mitigate catastrophic forgetting. Additionally, \adaptiverl 
supports seamless incorporation of new actions into the agentâ€™s action space. We validate our approach 
on a standard RL benchmark with shifting reward functions and the introduction of new action. 
Additionally we validate our approach in a realistic traffic signal control application to manage road 
intersections. The results demonstrate the proposed approach evidences superior convergence 
speed, resource efficiency, and continuous adaptation to evolving conditions, with respect to the 
baseline.
\end{abstract}



\begin{IEEEkeywords}
Reinforcement Learning,  
Continual Reinforcement Learning,  
Q-learning,  
Concept Drift Detection,  
Adaptive Systems,  
Traffic Signal Control
\end{IEEEkeywords}

\input{introduction}
\input{background}
\input{implementation}
\input{validation}
\input{related}
\input{conclusion}

%\section*{Acknowledgment}

\printbibliography

\end{document}
