% !TeX document-id = {4951685f-4d1a-4629-8de6-69a05c8522c0}
%  $Id: main.tex $
% !BIB TS-program = bibtex

\RequirePackage{graphicx}
\documentclass[10pt, conference, ]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\input{preamble}
\input{acronyms}


\begin{document}

\title{Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions}

%\author{
%\IEEEauthorblockN{Raul de la Rosa}
%\IEEEauthorblockA{%\textit{ Department of Computer Science and Technology} \\
%\textit{Universidad de los Andes}\\
%Bogot\'a, Colombia \\
%c.delarosap@uniandes.edu.co}
%\and
%\IEEEauthorblockN{Ivana Dusparic}
%\IEEEauthorblockA{%\textit{School of Computer Science and Statistics} \\
%\textit{Trinity College Dublin}\\
%Dublin, Ireland \\
%ivana.dusparic@tcd.ie}
%\and
%\IEEEauthorblockN{Nicol\'as Cardozo}
%\IEEEauthorblockA{%\textit{Systems and Computing Engineering Department}\\
%\textit{Universidad de los Andes}\\
%Bogot\'a, Colombia \\
%n.cardozo@uniandes.edu.co}
%}
\author{
\IEEEauthorblockN{Author template}
\IEEEauthorblockA{%\textit{ Department of Computer Science and Technology} \\
\textit{University}\\
City, Country \\
university-email}
}

\maketitle




\begin{abstract}
Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces \adaptiverl, a self-adaptive Q-learning framework that enables on-the-fly  adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, \adaptiverl adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to mitigate catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that \adaptiverl achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.9x.
\end{abstract}



\begin{IEEEkeywords}
Reinforcement Learning,  
Continual Reinforcement Learning,  
Q-learning,  
Concept Drift Detection,  
Adaptive Systems,  
Traffic Signal Control
\end{IEEEkeywords}

\input{introduction}
\input{background}
\input{implementation}
\input{validation}
\input{related}
\input{conclusion}

%\section*{Acknowledgment}
%I thank my teachers and the university for their scholarship %support that made this work possible. As a non-native English %speaker. 

\printbibliography

\end{document}
