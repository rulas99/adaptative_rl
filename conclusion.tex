% $Id: conclusion.tex 
% !TEX root = main.tex

%%
\section{Conclusion and Future Work}
\label{sec:conclusion}

This paper introduced \adaptiverl, a self-adaptive framework for tabular Q-learning agents operating in non-stationary environments. We specifically addressed the challenge of agents adapting to simultaneous changes in their goals (reward functions) and their available capabilities (action-space expansions). Our approach integrates concept drift detection using the PH-test with dynamic adjustments to the exploration ($\varepsilon$) and learning ($\alpha$) rates. Experimental results in both a Gridworld benchmark and a traffic control simulation demonstrate that this coordinated strategy enables agents to adapt more effectively than a standard Q-learning baseline, achieving a performance increase of up to $1.7\times$ in learning efficiency while successfully reducing catastrophic forgetting effects.

Through this synthesis of established techniques into a unified framework, \adaptiverl demonstrates how a lightweight, model-free agent can achieve robust continual learning. By preserving and adapting a single Q-table, it effectively enables knowledge reuse when faced with contradictory environmental changes, a key challenge highlighted by~\citet{Bagus2022}, without requiring full retraining or multiple context models. Our work thus presents a practical and resource-efficient method for developing self-adaptive systems capable of real-time learning in dynamic environments.

This work represents an initial validation, and several avenues for future work are evident. The limitations observed in our experiments, such as the failure of the PH-test to detect certain drifts and the need for empirical tuning of hyperparameters, point to clear directions for improvement. Future work should therefore focus on:
\begin{enumerate}
    \item Generalization and Scalability: Extending the core principles of \adaptiverl from tabular methods to deep \ac{RL} architectures to handle high-dimensional state spaces. This would also involve investigating the use of memory-based plasticity to enhance knowledge transfer across tasks.
    \item Robustness and Empirical Analysis: Conducting a rigorous sensitivity analysis of the introduced hyperparameters, particularly the TD-error sensitivity ($k$) and $H$ threshold for the PH-test, to better understand their impact on performance. Furthermore, we plan to explore more robust drift detection methods to overcome the limitations of the PH-test observed in our traffic scenario, where drifts that result in subsets of known reward distributions can be missed. We also intend to extend the evaluation to include scenarios with action removal and other types of reward function changes, such as those examined by \citet{mignon2017adaptive}.
    \item Advanced Application Domains: Applying the framework to more complex distributed multi-agent systems (\eg bigger street network configuration). Finally, we plan to deploy \adaptiverl on resource-constrained edge devices for real-world applications in \ac{IOT} and robotics.
\end{enumerate}

\endinput