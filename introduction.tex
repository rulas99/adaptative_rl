% $Id: introduction.tex $
% !TEX root = main.tex

\section{Introduction}
\label{sec:introduction}

Traditional \acf{RL} algorithms assume stationary \acp{MDP}, where transition probabilities and reward functions remain constant~\cite{sutton18}. However, real-world environments often exhibit non-stationary characteristics like evolving reward dynamics and changing action spaces~\cite{khetarpal2022continualreinforcementlearningreview}, limiting the effectiveness of standard approaches.

This paper introduces \adaptiverl, a self-adaptive Q-learning algorithm that addresses two key challenges in non-stationary environments: (1) adapting to changing reward functions (goals), and (2) incorporating new actions into the agent's behavior. Our approach integrates concept drift\footnote{Concept drift refers to a change in the environment's underlying \ac{MDP}, where the dynamics shift. This implies that the agent's previously learned policy and Q-values are inaccurate for maximizing future rewards.} detection using the Page-Hinkley test (PH-test)\cite{changingpointdetection} with dynamic adjustment of learning ($\alpha$) and exploration ($\varepsilon$) rates, enabling agents to explore for enough time in order to preserve prior knowledge \cite{norman2024firstexploreexploitmetalearningsolve} while adapting to environmental changes.

\adaptiverl operates through two main mechanisms: \emph{environment monitoring} for drift detection, and \emph{adaptive learning} that dynamically adjusts parameters based on temporal difference errors. When concept drift is detected, the agent increases exploration until the agents stabilize over the new configuration while maintaining its Q-table structure, allowing rapid adaptation without catastrophic forgetting. For new actions, the agent extends its Q-table dimensions and applies targeted exploration to integrate new capabilities.

We validate our approach on Gridworld benchmarks with shifting goals and expanding action spaces, and demonstrate practical applicability in traffic signal control scenarios. Results show superior convergence speed and adaptation efficiency compared to standard Q-learning baselines, positioning \adaptiverl as a suitable approach for self-adaptive systems operating in dynamic environments.
\endinput

