% $Id: implementation.tex 
% !TEX root = main.tex

%%
\section{\ac{RL} Agents with Adaptive Behavior}
\label{sec:implementation}

This section introduces \adaptiverl, a self-adaptive Q-learning framework that enables agents to dynamically adapt to non-stationary environments, specifically to changes in reward functions (goals) and on-the-fly expansions of the action space. \adaptiverl integrates two core components, detailed in Algorithm \ref{alg:morphin}: proactive environment monitoring using a concept drift detector, and an adaptive learning process that modulates exploration and learning rates to incorporate new knowledge while preserving prior experience. The implementation is publicly available.\footnote{Available at: \url{https://anonymous.4open.science/r/morphin_rl}} This approach aligns with the principles of \ac{CRL}~\cite{abel2023definitioncontinualreinforcementlearning} and Self-Adaptive Systems (SAS)~\cite{WONG2022106934} by enabling agents to autonomously modify their learning strategy at runtime, ensuring resilience in non-stationary contexts.

\begin{algorithm}[hbt!]
\caption{\adaptiverl: Adaptive Q-Learning with Concept Drift Detection}
\label{alg:morphin}
\begin{algorithmic}[1]
\State \textbf{Initialize} parameters:
\State \quad Base learning rate $\alpha$, max learning rate $\alpha_{\max}$, discount factor $\gamma$
\State \quad TD-error sensitivity $k$, exploration decay parameters $\varepsilon_{\min}, \text{decay\_rate}$
\State \quad Page-Hinkley parameters: sensitivity $\delta$, threshold $H$
\State \textbf{Initialize} Q-table $Q(s, a) \leftarrow 0$ for all $s \in S, a \in A$
\State \textbf{Initialize} drift detector $PH\_Test(\delta, H)$
\State \textbf{Initialize} exploration decay counter $e \leftarrow 0$

\For{episode = 1 to N}
    \State Reset state $s_t \leftarrow s_{\text{initial}}$
    \State Reset cumulative episode reward $R_{ep} \leftarrow 0$
    
    \While{$s_t$ is not terminal}
        \State $\varepsilon_t \leftarrow \varepsilon_{\min} + (1 - \varepsilon_{\min}) \cdot \exp(-\text{decay\_rate} \cdot e)$
        \State $a_t \leftarrow \text{choose\_action}(s_t, \varepsilon_t)$ \Comment{$\varepsilon$-greedy selection}
        \State Execute $a_t$, observe $r_{t+1}$ and $s_{t+1}$
        \State $R_{ep} \leftarrow R_{ep} + r_{t+1}$
        \State\Comment{Adaptively update Q-value with eq. \eqref{eq:td_error} \& \eqref{eq:dynamic_learning_rate}}
        \State $Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha^* \cdot TD_{error}$
        \State $s_t \leftarrow s_{t+1}$
    \EndWhile
    
    \State\Comment{Detect Concept Drift at the end of the episode}
    \If{$PH\_Test.update(R_{ep})$ is True}
        \State \Comment{Drift detected: reset exploration schedule}
        \State $e \leftarrow 0$
        \State $PH\_Test.reset()$
    \Else
        \State \Comment{Stable environment: continue exploration decay}
        \State $e \leftarrow e + 1$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Environment Monitoring and Adaptation}
\label{sec:morphin-adaptation}

To adapt, an agent must first recognize environmental changes. \adaptiverl achieves this by monitoring the stream of cumulative episode rewards ($R_{ep}$) with the PH-test~\cite{mignon2017adaptive,networkdynamicrl,changingpointdetection}. A drift is flagged if the cumulative difference between $R_{ep}$ and its running mean exceeds a threshold $H$. The test's sensitivity is controlled by $H$ and a second hyperparameter, $\delta$, which were selected empirically based on the reward scale of each environment. As shown in Algorithm \ref{alg:morphin} (lines 21--28), a detected drift triggers an immediate adaptive response: the exploration decay counter $e$ is reset to zero, forcing the exploration rate $\varepsilon^*$ to its maximum value. This compels the agent to re-explore the environment and learn the new dynamics, as visualized in \fref{fig:morphin-dynamics}.

Once a drift is detected, \adaptiverl employs a two-points strategy. The first part is the mandatory re-exploration described above. The second is a dynamic adjustment of the learning rate $\alpha^*$ to control the speed of knowledge acquisition. This dual mechanism is a cornerstone of \adaptiverl, designed to preserve prior knowledge and mitigate catastrophic forgetting. Unlike approaches that retrain from scratch, \adaptiverl never resets the Q-table; existing Q-values serve as an informed starting point for learning the new policy. This reuse of knowledge enables faster adaptation, as evidenced by the shorter re-learning periods showed in \fref{fig:morphin-dynamics}.

The learning rate adaptation is driven by the Temporal Difference (TD) error, which quantifies the discrepancy between the predicted and actual outcomes of an action:
\begin{equation} \label{eq:td_error}
    TD_{error} = r_{t+1} + \gamma \cdot \underset{a}{\max} Q(s_{t+1}, a) - Q(s_t, a_t)
\end{equation}
A concept drift leads to large TD-errors, signaling a mismatch between the agent's knowledge and the new reality. \adaptiverl harnesses this signal to compute a dynamic learning rate $\alpha^*$:
\begin{equation}
    \label{eq:dynamic_learning_rate}
    \alpha^* = \alpha + (\alpha_{\max}-\alpha) \cdot \frac{1}{1 + e^{-(|TD_{error}|-k)}}
\end{equation}
Here, the hyperparameter $k$ controls the sensitivity of $\alpha^*$ to the TD-error and was empirically tuned. High TD-errors yield a large $\alpha^*$, accelerating learning. As the policy stabilizes, TD-errors diminish and $\alpha^*$ decays towards its base value $\alpha$, ensuring convergence.

This framework also handles an expanding action space. When a new action $a_{new}$ becomes available, the Q-table (a matrix of size $|A| \times |S|$) is dynamically augmented by adding a new row for $a_{new}$, initialized to zero. This event, detected as a drift, triggers the same unified response: $\varepsilon^*$ is reset to explore the new action's utility, and $\alpha^*$ adapts to integrate it into the policy.

\begin{figure*}[hbtp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/eps_alph.png}
        \caption{\adaptiverl with adaptive $\varepsilon^*$ and $\alpha^*$.}
        \label{fig:morphin-dynamics}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/trad_eps.png}
        \caption{Standard Q-learning with static $\varepsilon$-decay.}
        \label{fig:baseline-dynamics}
\end{subfigure}
    
    \caption{Internal dynamics of \adaptiverl versus a standard Q-learning agent, illustrating their contrasting responses to concept drift (red dashed lines) in the Gridworld scenario. (a) \adaptiverl's adaptive response: Upon detecting a drift, the PH-test triggers a reset of the exploration rate $\varepsilon^*$ (purple), forcing re-exploration. The resulting high TD-errors cause the dynamic learning rate $\alpha^*$ (green) to increase, accelerating the integration of new knowledge and enabling rapid policy recovery. (b) Standard Q-learning's static behavior: The baseline agent uses a single, exponentially decaying exploration schedule ($\varepsilon$). After the initial convergence, the low exploration rate prevents it from adapting to subsequent drifts, causing the agent to remain committed to an obsolete policy and resulting in a sustained performance collapse.}
    \label{fig:dynamics-combined}
\end{figure*}

The effectiveness of \adaptiverl becomes evident when contrasted with a standard Q-learning agent (\fref{fig:baseline-dynamics}). The baseline agent's static exploration schedule prevents adaptation to new goals, causing a sustained drop in performance. In sharp contrast, \adaptiverl (\fref{fig:morphin-dynamics}) explicitly detects changes and coordinates exploration ($\varepsilon^*$) with learning speed ($\alpha^*$) to rapidly converge to a new optimal policy after each drift.

\endinput