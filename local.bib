@comment{----2024 ---}

@misc{norman2024firstexploreexploitmetalearningsolve,
  	title={First-Explore, then Exploit: Meta-Learning to Solve Hard Exploration-Exploitation Trade-Offs},
  	author={Ben Norman and Jeff Clune},
  	year={2024},
  	eprint={2307.02276},
  	archivePrefix={arXiv},
  	primaryClass={cs.LG},
  	url={https://arxiv.org/abs/2307.02276}
}

@misc{abel2023definitioncontinualreinforcementlearning,
  	title={A Definition of Continual Reinforcement Learning},
  	author={David Abel and André Barreto and Benjamin Van Roy and Doina Precup and Hado van Hasselt and Satinder Singh},
  	year={2023},
  	eprint={2307.11046},
  	archivePrefix={arXiv},
  	primaryClass={cs.LG},
  	url={https://arxiv.org/abs/2307.11046},
}

@article{HENRICHS2022106940,
title = {A literature review on optimization techniques for adaptation planning in adaptive systems: State of the art and research directions},
journal = {Information and Software Technology},
volume = {149},
pages = {106940},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106940},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000891},
author = {Elia Henrichs and Veronika Lesch and Martin Straesser and Samuel Kounev and Christian Krupitzer},
keywords = {Self-adaptive systems, Adaptation planning, Optimization, Survey},
}

@misc{khetarpal2022continualreinforcementlearningreview,
      title={Towards Continual Reinforcement Learning: A Review and Perspectives}, 
      author={Khimya Khetarpal and Matthew Riemer and Irina Rish and Doina Precup},
      year={2022},
      eprint={2012.13490},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.13490}, 
}

@misc{chen2022transferredqlearning,
      title={Transferred Q-learning}, 
      author={Elynn Y. Chen and Michael I. Jordan and Sai Li},
      year={2022},
      eprint={2202.04709},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.04709}, 
}

@misc{beck2024surveymetareinforcementlearning,
      title={A Survey of Meta-Reinforcement Learning}, 
      author={Jacob Beck and Risto Vuorio and Evan Zheran Liu and Zheng Xiong and Luisa Zintgraf and Chelsea Finn and Shimon Whiteson},
      year={2024},
      eprint={2301.08028},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.08028}, 
}

@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@article{meta-rl-traffic,
author = {Kim, Gyeongjun and Kang, Jiwon and Sohn, Keemin},
year = {2022},
month = {09},
pages = {},
title = {A meta–reinforcement learning algorithm for traffic signal control to automatically switch different reward functions according to the saturation level of traffic flows},
volume = {38},
journal = {Computer-Aided Civil and Infrastructure Engineering},
doi = {10.1111/mice.12924}
}

@unknown{dynamicrlalpha,
author = {Donancio, Henrique and Barrier, Antoine and South, Leah and Forbes, Florence},
year = {2024},
month = {10},
pages = {},
title = {Dynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach},
doi = {10.48550/arXiv.2410.12598}
}


@article{MORENOMALO2024124178,
title = {Improving traffic light systems using Deep Q-networks},
journal = {Expert Systems with Applications},
volume = {252},
pages = {124178},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124178},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424010443},
author = {Juan Moreno-Malo and Juan-Luis Posadas-Yagüe and Juan Carlos Cano and Carlos T. Calafate and J. Alberto Conejero and Jose-Luis Poza-Lujan},
keywords = {Deep-Q networks, Urban traffic optimization, Neural network, SUMO},
abstract = {As our cities become more complex and traffic demand grows, managing such traffic efficiently becomes challenging. Hence, solutions that allow building upon the current traffic light systems and that can be readily deployed are of global interest. In this work, we address the challenge of improving traffic light management at intersections. We propose an agent-based traffic light control system where an agent, one per intersection, dynamically regulates the light’s phase cycle depending on the current traffic conditions. To this end, we will rely on Deep Networks to adequately train agents to make good decisions. Simulation results in a realistic scenario using SUMO show that our proposed approach can significantly reduce waiting times, improving transit times by 44% compared to the standard fixed-timing method. Additionally, to assess the effectiveness and reliability of our control algorithm, we introduce new performance metrics.}
}

@inproceedings{tokic2010,
author = {Tokic, Michel},
year = {2010},
month = {09},
pages = {203-210},
title = {Adaptive ε-Greedy Exploration in Reinforcement Learning Based on Value Differences},
isbn = {978-3-642-16110-0},
doi = {10.1007/978-3-642-16111-7_23}
}

@article{mignon2017adaptive,
author = {Mignon, Alexandre and A. Rocha, Ricardo Luis},
year = {2017},
month = {12},
pages = {1146-1151},
title = {An Adaptive Implementation of ε-Greedy in Reinforcement Learning},
volume = {109},
journal = {Procedia Computer Science},
doi = {10.1016/j.procs.2017.05.431}
}

@inproceedings{inproceedings,
author = {Wassermann, Sarah and Cuvelier, Thibaut and Mulinka, Pavol and Casas, Pedro},
year = {2019},
month = {10},
pages = {},
title = {ADAM & RAL: Adaptive Memory Learning and Reinforcement Active Learning for Network Monitoring},
doi = {10.23919/CNSM46954.2019.9012675}
}

@article{networkdynamicrl,
author = {Wassermann, Sarah and Cuvelier, Thibaut and Mulinka, Pavol and Casas, Pedro},
year = {2020},
month = {11},
pages = {1-1},
title = {Adaptive and Reinforcement Learning Approaches for Online Network Monitoring and Analysis},
volume = {PP},
journal = {IEEE Transactions on Network and Service Management},
doi = {10.1109/TNSM.2020.3037486}
}

@article{iotdynamicrl,
author = {Kumar, Anit and Singh, Dhanpratap},
year = {2024},
month = {11},
pages = {},
title = {Adaptive epsilon greedy reinforcement learning method in securing IoT devices in edge computing},
volume = {4},
journal = {Discover Internet of Things},
doi = {10.1007/s43926-024-00080-7}
}

@inproceedings{10.1609/aaai.v37i6.25899,
author = {Ding, Wei and Jiang, Siyang and Chen, Hsi-Wen and Chen, Ming-Syan},
title = {Incremental reinforcement learning with dual-adaptive ε-greedy exploration},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i6.25899},
doi = {10.1609/aaai.v37i6.25899},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {830},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@Article{Swapno2024,
author={Swapno, S. M. Masfequier Rahman
and Nobel, SM Nuruzzaman
and Meena, Preeti
and Meena, V. P.
and Azar, Ahmad Taher
and Haider, Zeeshan
and Tounsi, Mohamed},
title={A reinforcement learning approach for reducing traffic congestion using deep Q learning},
journal={Scientific Reports},
year={2024},
month={Dec},
day={12},
volume={14},
number={1},
pages={30452},
issn={2045-2322},
doi={10.1038/s41598-024-75638-0},
url={https://doi.org/10.1038/s41598-024-75638-0}
}

@inproceedings{zintgraf21,
  title={Exploration in approximate hyper-state space for meta reinforcement learning},
  author={Zintgraf, Luisa M and Feng, Leo and Lu, Cong and Igl, Maximilian and Hartikainen, Kristian and Hofmann, Katja and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={12991--13001},
  year={2021},
  organization={PMLR}
}

@article{zhuang20,
author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
year = {2020},
month = {07},
pages = {1-34},
title = {A Comprehensive Survey on Transfer Learning},
volume = {PP},
journal = {Proceedings of the IEEE},
doi = {10.1109/JPROC.2020.3004555}
}

@misc{sasreview,
      title={Self-Adaptive Systems: A Systematic Literature Review Across Categories and Domains}, 
      author={Terence Wong and Markus Wagner and Christoph Treude},
      year={2022},
      eprint={2101.00125},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2101.00125}, 
}

@misc{araujo2020controladaptiveqlearning,
      title={Control with adaptive Q-learning}, 
      author={João Pedro Araújo and Mário A. T. Figueiredo and Miguel Ayala Botto},
      year={2020},
      eprint={2011.02141},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.02141}, 
}

@inproceedings{Bagus2022,
   title={A Study of Continual Learning Methods for Q-Learning},
   url={http://dx.doi.org/10.1109/IJCNN55064.2022.9892384},
   DOI={10.1109/ijcnn55064.2022.9892384},
   booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
   publisher={IEEE},
   author={Bagus, Benedikt and Gepperth, Alexander},
   year={2022},
   month={07}, 
   pages={1–9} 
}

@misc{towers2024gymnasiumstandardinterfacereinforcement,
      title={Gymnasium: A Standard Interface for Reinforcement Learning Environments}, 
      author={Mark Towers and Ariel Kwiatkowski and Jordan Terry and John U. Balis and Gianluca De Cola and Tristan Deleu and Manuel Goulão and Andreas Kallinteris and Markus Krimmel and Arjun KG and Rodrigo Perez-Vicente and Andrea Pierré and Sander Schulhoff and Jun Jet Tai and Hannah Tan and Omar G. Younis},
      year={2024},
      eprint={2407.17032},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.17032}, 
}